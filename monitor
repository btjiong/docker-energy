#!/bin/bash

# Get the arguments
while getopts "b:n:i:t:" arg; do
  case $arg in
    b) BASE=$OPTARG;;
    n) NUMBER=$OPTARG;;
    i) INFERENCE=$OPTARG;;
    t) TOKENS=$OPTARG;;
    *) ;;
  esac
done

# Default values
BASE=${BASE:-ubuntu}
NUMBER=${NUMBER:-1}
INFERENCE=${INFERENCE:-"Building a website can be done in 10 simple steps:"}
TOKENS=${TOKENS:-512}

. llama.cpp/build_image -b "${BASE}"

# Warm up workload (prime number test)
date +"# started on %c" >> logs/llama.cpp_"${BASE}".txt
sysbench cpu --time=10 --cpu-max-prime=100000 --threads=2 run | tee -a logs/llama.cpp_"${BASE}".txt

# Run and monitor the workload for the specified number of times
for ((i=1; i<=NUMBER; i++)); do
  sleep 10
  perf stat -o results/llama.cpp_"${BASE}".txt --append -e power/energy-cores/,power/energy-ram/,power/energy-gpu/,power/energy-pkg/ \
  docker run --name llama.cpp-"${BASE}" -v "${PWD}"/llama.cpp/models:/models llama.cpp-"${BASE}" -m /models/7B/ggml-model-q4_0.bin -p "${INFERENCE}" -n "${TOKENS}" --seed 12345678
  docker logs -f llama.cpp-"${BASE}" &>> logs/llama.cpp_"${BASE}".txt
  echo -e "\n\n" >> logs/llama.cpp_"${BASE}".txt
  docker rm llama.cpp-"${BASE}"
done

docker rmi llama.cpp-"${BASE}"