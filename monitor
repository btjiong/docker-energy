#!/bin/bash

# Get the arguments
while getopts "b:n:i:t:u:" arg; do
  case $arg in
    b) BASE=$OPTARG;;
    n) NUMBER=$OPTARG;;
    i) INFERENCE=$OPTARG;;
    t) TOKENS=$OPTARG;;
    u) RUNID=$OPTARG;;
    *) ;;
  esac
done

# Default values
BASE=${BASE:-ubuntu}
NUMBER=${NUMBER:-1}
INFERENCE=${INFERENCE:-"Building a website can be done in 10 simple steps:"}
TOKENS=${TOKENS:-512}
RUNID=${RUNID:--1}

. prepare -u "${RUNID}" -b "${BASE}"

# Warm up workload (prime number test)
# . warm_up

# Run and monitor the workload for the specified number of times
for ((i=1; i<=NUMBER; i++)); do
  sleep 10
  echo -e "# run ${i}" >> results/llama.cpp_"${BASE}".txt
  perf stat -o results/llama.cpp_"${BASE}".txt --append -e power/energy-cores/,power/energy-ram/,power/energy-gpu/,power/energy-pkg/ \
  docker run --name llama.cpp-"${BASE}" -v "${PWD}"/llama.cpp/models:/models llama.cpp-"${BASE}" -m /models/7B/ggml-model-q4_0.bin -p "${INFERENCE}" -n "${TOKENS}" --seed 12345678
  date +"# run ${i}%n# started on %c %n" >> logs/llama.cpp_"${BASE}".txt
  docker logs -f llama.cpp-"${BASE}" &>> logs/llama.cpp_"${BASE}".txt
  echo -e "\n\n" | tee -a logs/llama.cpp_"${BASE}".txt results/llama.cpp_"${BASE}".txt
  docker rm llama.cpp-"${BASE}"
done

docker rmi llama.cpp-"${BASE}"