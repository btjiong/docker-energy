name: 'llama.cpp-model'
description: 'Inference of LLaMA model in pure C/C++; similar to the llama.cpp workload, but this workload includes the model in the container instead of using volumes'
development: false
cpus: 2
docker: true
images:
    - 'ubuntu:22.04'
    - 'debian:12.0'
    - 'alpine:3.18.2'
    - 'centos:8.4.2105'
