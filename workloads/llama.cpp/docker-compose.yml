services:
    llama.cpp:
        image: llama.cpp-${NAME}
        container_name: llama.cpp
        cpuset: ${ISOLATE_CPU}
        build:
            context: '$PWD/workloads/llama.cpp/'
            dockerfile: '$PWD/workloads/llama.cpp/${FILE}'
            args:
                - BASE=${IMAGE}
        volumes:
            - '${PWD}/llama.cpp/models:/models'
        command: -m /models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 1024 --seed 12345678 -t '${THREADS_CPU}'
