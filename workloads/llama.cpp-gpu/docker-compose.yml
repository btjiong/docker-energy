services:
    llama.cpp:
        image: llama.cpp-${NAME}
        container_name: llama.cpp-gpu
        cpuset: ${ISOLATE_CPU}
        build:
            context: '${PWD}/llama.cpp/'
            dockerfile: '$PWD/workloads/llama.cpp/${FILE}'
            args:
                - BASE=${IMAGE}
        volumes:
            - '${PWD}/llama.cpp/models:/models'
        command: -m /models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 1024 --seed 12345678 -t '${THREADS_CPU}' --n-gpu-layers 99
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]